import json 
import re 
import numpy as np 
from collections import Counter 
from tqdm import tqdm
from openai import OpenAI
import anthropic
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

'''
loads scored_tiles.json generated by score_tiles.py and outputs to Lllm_evaluation.json a classification of either 
"Likely Techosignature" or "Likely RFI - Noise / artifact" along with an explanation for each entry.

Currently can be used with chatgpt or Qwen, although there are some errors in the Qwen output, these are a work in progress, claude is in place but not yet tested.

Adjust HEURISTICS as needed based on data, also needs improvement to prompt and explanations--updates coming
'''

#client = OpenAI() # uncomment if using chatGPT

# config
INPUT_JSON = "../outputs/scored_tiles.json" 
OUTPUT_JSON = "../outputs/Lllm_evaluation.json" 
LLM_NAME = "local" # "chatgpt", "claude", "local" 
MODEL_MAP = {
    "chatgpt": "gpt-4o-mini",
    "claude": "claude-3-sonnet-20240229",
}

NUM_REPEATS = 3 # consistency metric
#Heuristic thresholds (simple + interpretable)
HEURISTICS = { 
    "max_power": 6.0, 
    "kurtosis": 8.0, 
    "entropy": 3.2
}

# define prompt
def build_prompt(tile): 
   f = tile["features"] 
   prompt = f""" 
   Hello! I would like you to evaluate candidate radio signals for technosignature searches. 

   Candidate properties: 
   CNN anomaly score: {tile['cnn_score']:.2f} 
   Mean normalized power: {f['mean']:.2f} 
   Standard deviation: {f['std']:.2f} 
   Maximum normalized power: {f['max']:.2f} 
   Kurtosis: {f['kurtosis']:.2f} 
   Spectral entropy: {f['entropy']:.2f} 

   Tasks: 
   1. Classify the signal as one of:
      - Likely technosignature 
      - Likely RFI - Noise / artifact 
   2. Briefly explain your reasoning. 

   Please respond in the following format: 
   Classification: <label> 
   Explanation: <text> 
   """
   return prompt.strip()

# call LLM
LOCAL_MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto"
)

def call_llm_local(prompt, max_new_tokens=300):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def call_llm(prompt):
    if LLM_NAME == "chatgpt":
        response = client.chat.completions.create(
            model=MODEL_MAP[LLM_NAME],
            messages=[
                {"role": "system", "content": "You are an expert radio astronomer."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            max_tokens=300,
        )
        return response.choices[0].message.content

    elif LLM_NAME == "claude":
        client_a = anthropic.Anthropic()
        response = client_a.messages.create(
            model=MODEL_MAP[LLM_NAME],
            max_tokens=300,
            temperature=0.2,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.content[0].text


    elif LLM_NAME == "local":
        return call_llm_local(prompt)


    else:
        raise ValueError(f"Unknown LLM_NAME: {LLM_NAME}")


# get response 
def parse_response(text): 
    label_match = re.search(r"Classification:\s*(.*)", text) 
    explanation_match = re.search(r"Explanation:\s*(.*)", text, re.DOTALL) 

    label = label_match.group(1).strip() if label_match else "Unknown" 
    explanation = explanation_match.group(1).strip() if explanation_match else "" 

    return label, explanation

# score llm response
def heuristic_label(tile): 
    f = tile["features"] 
    if ( 
        f["max"] >  HEURISTICS["max_power"] 
        and f["kurtosis"] > HEURISTICS["kurtosis"] 
        and f["entropy"] < HEURISTICS["entropy"]
        ): 
        return "Likely technosignature" 
    return "Likely RFI / noise" 

def explanation_score(text):
    """ 
    counts how many concepts are referenced in the explanation 
    """ 
    keywords = ["drift", "narrow", "power", "bandwidth", "noise", "rfi"] 
    return sum(k in text.lower() for k in keywords)

# llm eval
with open(INPUT_JSON) as f: 
    tiles = json.load(f) 

# sanity check, does it work
print("Running single-tile LLM sanity check...")
print(call_llm(build_prompt(tiles[0])))
print("=" * 80)

# full call
results = [] 
for tile in tqdm(tiles):
    prompt = build_prompt(tile) 
    responses = [] 
    labels = [] 
    explanation_scores = [] 
    for _ in range(NUM_REPEATS):
        text = call_llm(prompt) 
        label, explanation = parse_response(text) 
        responses.append(text) 
        labels.append(label) 
        explanation_scores.append(explanation_score(explanation))

    # Consistency = majority vote fraction
    label_counts = Counter(labels) 
    majority_label, majority_count = label_counts.most_common(1)[0] 
    consistency = majority_count / NUM_REPEATS 
    heuristic = heuristic_label(tile) 
    agreement = majority_label == heuristic 
    results.append({
        "tile_id": tile["tile_id"], 
        "cnn_score": tile["cnn_score"], 
        "heuristic_label": heuristic, 
        "llm_majority_label": majority_label, 
        "consistency": consistency, 
        "heuristic_agreement": agreement, 
        "mean_explanation_score": float(np.mean(explanation_scores)), 
        "raw_responses": responses
    })

with open(OUTPUT_JSON, "w") as f: json.dump(results, f, indent=2) 
print(f"Saved LLM evaluation to {OUTPUT_JSON}")

